{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f3ef1c-6fde-4129-b970-acac83ab9c7d",
   "metadata": {},
   "source": [
    "# Comet ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe1a43f-0bc9-480a-9b4a-9dd2d58d7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ export COMET_API_KEY=\"ZNgNJ1VVgmaAbL0ga1t4mw3JI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da014dee-fee0-4bea-8fc0-df9c5fc57b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !comet check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71a53c6-eee5-48db-b81b-d40745ca82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import comet_ml\n",
    "\n",
    "# comet_ml.init(project_name=\"roberta-base-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f2734-2bcb-4ed9-a47c-ef27065eceeb",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69a1ed5-07eb-4e64-b17f-53271809fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faddd3f5-970f-4f8f-88ac-e2f25513cf88",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a79ecd8-f512-4ec9-b382-b5fb5ad4d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smooth/miniconda3/envs/ner/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74dd48-fd4d-426e-8d00-ea8c7934ca6e",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34200f88-40a7-474f-a4f0-66b05fc7265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-MAT', 'I-MAT', 'B-NMAT', 'I-NMAT', 'B-DIMENSION', 'I-DIMENSION', 'B-WEIGHT', 'I-WEIGHT', 'B-TARGET_USER', 'I-TARGET_USER', 'B-PROPERTY', 'I-PROPERTY', 'B-COLOR', 'I-COLOR', 'B-SHAPE', 'I-SHAPE', 'B-SIZE', 'I-SIZE']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "ner_tags = [\n",
    "\"MAT\",\n",
    "\"NMAT\",\n",
    "\"DIMENSION\",\n",
    "\"WEIGHT\",\n",
    "\"TARGET_USER\",\n",
    "\"PROPERTY\",\n",
    "\"COLOR\",\n",
    "\"SHAPE\",\n",
    "\"SIZE\",\n",
    "]\n",
    "\n",
    "processed_ner_tags = ['O']\n",
    "for tag in ner_tags:\n",
    "        processed_ner_tags.extend([f\"B-{tag}\", f\"I-{tag}\"])\n",
    "\n",
    "print(processed_ner_tags)\n",
    "print(len(processed_ner_tags))\n",
    "\n",
    "ner_tags_2_number = {t: i for (i, t) in enumerate(processed_ner_tags)}\n",
    "number_2_ner_tags = {t: i for (t, i) in enumerate(ner_tags_2_number)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c8e8ca-d9b3-4756-a8a7-b4090cd978be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3952 entries, 0 to 3951\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3952 non-null   object\n",
      " 1   locs      3952 non-null   object\n",
      " 2   words     3952 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 92.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_excel_dataset = pd.read_excel(\"../data/raw_data_restore_uppercase.xlsx\")\n",
    "raw_excel_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbe29a1-5e60-4785-85c1-9ed1564f5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def assign_ner_tags_roberta(example):\n",
    "    token_input = tokenizer(example['sentence'])\n",
    "    example['tokens'] = tokenizer.convert_ids_to_tokens(\n",
    "        token_input['input_ids'])\n",
    "\n",
    "    ner_tags = [0 for token in example['tokens']]\n",
    "    if str(type(example['locs'])) == \"<class 'list'>\":\n",
    "        locs = example['locs']\n",
    "    else:\n",
    "        locs = ast.literal_eval(example['locs'])\n",
    "\n",
    "    locs = [(int(loc[0]), int(loc[1]), loc[2]) for loc in locs]\n",
    "    locs = sorted(locs)\n",
    "    bg_id = 1\n",
    "    pre_loc = 0\n",
    "    text = example['sentence']\n",
    "    for loc in locs:\n",
    "        loc0 = int(loc[0])\n",
    "        loc1 = int(loc[1])\n",
    "\n",
    "        pre_text = text[pre_loc:loc0]\n",
    "        if 0 < loc0 and len(pre_text) > 0 and pre_text[-1] == ' ':\n",
    "            pre_text = text[pre_loc:loc0 - 1]\n",
    "        token_input = tokenizer(pre_text)\n",
    "        pre_token = tokenizer.convert_ids_to_tokens(\n",
    "            token_input['input_ids'])\n",
    "        bg_id = bg_id + len(pre_token) - 2\n",
    "        pre_loc = loc1\n",
    "\n",
    "        word = example['sentence'][loc0: loc1].strip()\n",
    "        if loc0 > 0 and example['sentence'][loc0 - 1] == ' ':\n",
    "            word = \" \" + word\n",
    "        token_input = tokenizer(word)\n",
    "        word_token = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "\n",
    "        label_number = ner_tags_2_number[f\"B-{loc[2]}\"]\n",
    "        ner_tags[bg_id] = label_number\n",
    "        bg_id += 1\n",
    "        for idx in range(bg_id, bg_id + len(word_token) - 3):\n",
    "            ner_tags[idx] = label_number + 1\n",
    "        bg_id = bg_id + len(word_token) - 3\n",
    "\n",
    "        # visualize_ner_tags(example['tokens'], ner_tags)\n",
    "\n",
    "    ner_tags[0] = -100\n",
    "    ner_tags[-1] = -100\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a20d79e-5537-41da-94b7-66c49d583a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Features, Value, Sequence\n",
    "\n",
    "list_input_ids = []\n",
    "list_attention_mask = []\n",
    "list_labels = []\n",
    "\n",
    "for index, row in raw_excel_dataset.iterrows():\n",
    "    try:\n",
    "        label = assign_ner_tags_roberta(row)\n",
    "        token_input = tokenizer(row['sentence'])\n",
    "        list_input_ids.append(token_input['input_ids'])\n",
    "        list_attention_mask.append(token_input['attention_mask'])\n",
    "        list_labels.append(label)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        break\n",
    "        print(index)\n",
    "        print(row['sentence'])\n",
    "\n",
    "tokenized_datasets = pd.DataFrame()\n",
    "tokenized_datasets['input_ids'] = pd.Series(list_input_ids)\n",
    "tokenized_datasets['attention_mask'] = pd.Series(list_attention_mask)\n",
    "tokenized_datasets['labels'] = pd.Series(list_labels)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "val_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "test_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "\n",
    "tokenized_datasets = DatasetDict(\n",
    "    {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d614cde-7b24-4b68-8329-b2ee7fcdd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=4,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc69c-2a5a-412d-ad6e-42b0a20ac31d",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74c316a-e2e6-435f-a596-92ebd61ad12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ed7d303-d16f-4018-9f69-6c3d286d0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.43653645e-02 4.08587006e+00 5.34026128e+00 1.58049209e+01\n",
      " 4.86108108e+01 6.38254081e+00 1.04679316e+00 2.35418848e+01\n",
      " 1.31862170e+01 6.54989075e+00 1.75988258e+01 1.85269881e+00\n",
      " 1.18735147e+00 2.25388471e+01 9.56702128e+01 3.84316239e+01\n",
      " 4.75820106e+01 5.76474359e+01 1.45048387e+02]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_class_weight(train_dataset, num_class):\n",
    "    list_label = []\n",
    "    for label in train_dataset['labels']:\n",
    "        for cl in label:\n",
    "            if cl != -100:\n",
    "                list_label.append(cl)\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=np.arange(num_class), y=list_label)\n",
    "    return class_weight\n",
    "\n",
    "class_weight = get_class_weight(train_dataset, len(processed_ner_tags))\n",
    "print(class_weight)\n",
    "print(len(class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b52a4-320b-46ed-8810-f20e01e03c34",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c08315b-93a9-4f4f-8c85-b13e93d0893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/wuharlem/simple-bert-w-hinge-loss\n",
    "\n",
    "from transformers import RobertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    " \n",
    "class CustomRoberta(RobertaForTokenClassification):\n",
    "  def __init__(self, config, class_weight=None):\n",
    "    super().__init__(config)\n",
    "    self.class_weight = torch.tensor(class_weight, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "  def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.class_weight is None:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(weight=self.class_weight)\n",
    "                \n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e7ca6-c11d-495e-8913-d46a638ea008",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b2399f-a635-41f4-81ac-904b94e3d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing CustomRoberta: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CustomRoberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomRoberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MAT\",\n",
      "    \"2\": \"I-MAT\",\n",
      "    \"3\": \"B-NMAT\",\n",
      "    \"4\": \"I-NMAT\",\n",
      "    \"5\": \"B-DIMENSION\",\n",
      "    \"6\": \"I-DIMENSION\",\n",
      "    \"7\": \"B-WEIGHT\",\n",
      "    \"8\": \"I-WEIGHT\",\n",
      "    \"9\": \"B-TARGET_USER\",\n",
      "    \"10\": \"I-TARGET_USER\",\n",
      "    \"11\": \"B-PROPERTY\",\n",
      "    \"12\": \"I-PROPERTY\",\n",
      "    \"13\": \"B-COLOR\",\n",
      "    \"14\": \"I-COLOR\",\n",
      "    \"15\": \"B-SHAPE\",\n",
      "    \"16\": \"I-SHAPE\",\n",
      "    \"17\": \"B-SIZE\",\n",
      "    \"18\": \"I-SIZE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-COLOR\": 13,\n",
      "    \"B-DIMENSION\": 5,\n",
      "    \"B-MAT\": 1,\n",
      "    \"B-NMAT\": 3,\n",
      "    \"B-PROPERTY\": 11,\n",
      "    \"B-SHAPE\": 15,\n",
      "    \"B-SIZE\": 17,\n",
      "    \"B-TARGET_USER\": 9,\n",
      "    \"B-WEIGHT\": 7,\n",
      "    \"I-COLOR\": 14,\n",
      "    \"I-DIMENSION\": 6,\n",
      "    \"I-MAT\": 2,\n",
      "    \"I-NMAT\": 4,\n",
      "    \"I-PROPERTY\": 12,\n",
      "    \"I-SHAPE\": 16,\n",
      "    \"I-SIZE\": 18,\n",
      "    \"I-TARGET_USER\": 10,\n",
      "    \"I-WEIGHT\": 8,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers.models.bert import modeling_bert\n",
    "from transformers import RobertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "\n",
    "label_names = processed_ner_tags\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = CustomRoberta.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3017f8bf-b894-4276-816d-34a3cb7e15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd464490-cfce-4968-a24b-8985eabb42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c381da-3959-4a0b-8699-cdc5bcc6e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/smooth/luunvt/attribute_extraction/notebooks' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/luunvt/roberta-base-ner-attribution-extraction/6a50fac91dd94653851024f12f69b059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment, ExistingExperiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "import comet_ml\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"ZNgNJ1VVgmaAbL0ga1t4mw3JI\",\n",
    "    project_name=\"roberta-base-ner-attribution-extraction\",\n",
    "    workspace=\"luunvt\",\n",
    "    log_code=True\n",
    ")\n",
    "\n",
    "hyper_params = {\n",
    "    \"model\": model_checkpoint, \n",
    "    \"num_epochs\": num_train_epochs, \n",
    "    \"optimizer\": \"adamW\",\n",
    "    \"use_class_weight\": True,\n",
    "    \"num_train_sample\": len(raw_excel_dataset),\n",
    "}\n",
    "experiment.log_parameters(hyper_params)\n",
    "experiment.add_tags([\"roberta-ner-classweight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0fb11-dd79-422a-82f7-3f883314e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2023-11-01\n",
      "Time: 16:37:04\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 0: {'precision': 0.5900492355532522, 'recall': 0.19347438185062452, 'f1': 0.2914000511901715, 'accuracy': 0.7495010739346978}\n",
      "Save best f1 model at epoch 0 with better f1 score 0.2914000511901715\n",
      "epoch 1: {'precision': 0.6998358814891595, 'recall': 0.31530199252801994, 'f1': 0.4347382824028117, 'accuracy': 0.8375929816758063}\n",
      "Save best f1 model at epoch 1 with better f1 score 0.4347382824028117\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "date_time = datetime.now()\n",
    "format_date = date_time.strftime('%Y-%m-%d')\n",
    "format_time = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "print(f\"Date: {format_date}\")\n",
    "print(f\"Time: {format_time}\")\n",
    "\n",
    "output_dir = f\"../models/model_from_{format_date}/deberta-base_{format_time}\"\n",
    "\n",
    "best_f1_score = 0\n",
    "    \n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(\n",
    "            predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(\n",
    "            labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(\n",
    "            predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if experiment:\n",
    "        experiment.set_epoch(epoch)\n",
    "        experiment.log_metric(\"precision\", results[\"overall_precision\"])\n",
    "        experiment.log_metric(\"recall\", results[\"overall_recall\"])\n",
    "        experiment.log_metric(\"f1\", results[\"overall_f1\"])\n",
    "        experiment.log_metric(\"accuracy\", results[\"overall_accuracy\"])\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process and results[f\"overall_f1\"] > best_f1_score:\n",
    "        output_ckpt = os.path.join(output_dir, f'best_f1')\n",
    "        best_f1_score = results[f\"overall_f1\"]\n",
    "        print(\n",
    "            f\"Save best f1 model at epoch {epoch} with better f1 score {best_f1_score}\")\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            output_dir, save_function=accelerator.save)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        output_ckpt = os.path.join(output_dir, f'epoch_{epoch + 1}')\n",
    "        print(f\"Save model at epoch {epoch + 1}\")\n",
    "        tokenizer.save_pretrained(output_ckpt)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            output_ckpt, save_function=accelerator.save)\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ner]",
   "language": "python",
   "name": "conda-env-ner-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
